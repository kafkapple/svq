training:
  num_epochs: 10
  optimizer:
    name: adam
    lr: 0.0001
    betas: [0.9, 0.999]
    weight_decay: 0.0001
    
  scheduler:
    name: cosine
    T_max: ${training.num_epochs}
    eta_min: 0.00001
    
  loss:
    reconstruction_weight: 1.0
    commitment_weight: 0.25
    perceptual_weight: 0.1
    disentanglement_weight: 0.1
    
  early_stopping:
    enabled: true
    patience: 10
    min_delta: 0.0001
    
  gradient_clipping:
    enabled: true
    max_norm: 1.0
    
  mixed_precision:
    enabled: true
    dtype: float16 